@article{deepcut,
    author = {Rajchl, Martin and Lee, Matthew C. H. and Oktay, Ozan and Kamnitsas, Konstantinos and Passerat-Palmbach, Jonathan and Bai, Wenjia and Damodaram, Mellisa and Rutherford, Mary A. and Hajnal, Joseph V. and Kainz, Bernhard and Rueckert, Daniel},
    journal = {IEEE Transactions on Medical Imaging}, 
    title = {DeepCut: Object Segmentation From Bounding Box Annotations Using Convolutional Neural Networks}, 
    year = {2017},
    volume = {36},
    number = {2},
    pages = {674-683},
    keywords = {Image segmentation;Training;Object segmentation;Biological neural networks;Optimization;Computational modeling;Imaging;Bounding box;convolutional neural networks;DeepCut;image segmentation;machine learning;weak annotations},
    doi = {10.1109/TMI.2016.2621185}}

@article{grabcut,
    author = {Rother, Carsten and Kolmogorov, Vladimir and Blake, Andrew},
    title = {``GrabCut'': interactive foreground extraction using iterated graph cuts},
    year = {2004},
    issue_date = {August 2004},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    volume = {23},
    number = {3},
    issn = {0730-0301},
    doi = {10.1145/1015706.1015720},
    abstract = {The problem of efficient, interactive foreground/background segmentation in still images is of great practical importance in image editing. Classical image segmentation tools use either texture (colour) information, e.g. Magic Wand, or edge (contrast) information, e.g. Intelligent Scissors. Recently, an approach based on optimization by graph-cut has been developed which successfully combines both types of information. In this paper we extend the graph-cut approach in three respects. First, we have developed a more powerful, iterative version of the optimisation. Secondly, the power of the iterative algorithm is used to simplify substantially the user interaction needed for a given quality of result. Thirdly, a robust algorithm for "border matting" has been developed to estimate simultaneously the alpha-matte around an object boundary and the colours of foreground pixels. We show that for moderately difficult examples the proposed method outperforms competitive tools.},
    journal = {ACM Trans. Graph.},
    month = {aug},
    pages = {309–314},
    numpages = {6},
    keywords = {Alpha Matting, Foreground extraction, Graph Cuts, Image Editing, Interactive Image Segmentation}
}


@inproceedings{crfrnn,
    author = {Zheng, Shuai and Jayasumana, Sadeep and Romera-Paredes, Bernardino and Vineet, Vibhav and Su, Zhizhong and Du, Dalong and Huang, Chang and Torr, Philip H. S.},
    booktitle = {2015 IEEE International Conference on Computer Vision (ICCV)}, 
    title = {Conditional Random Fields as Recurrent Neural Networks}, 
    year = {2015},
    volume = {},
    number = {},
    pages = {1529-1537},
    keywords = {Labeling;Image segmentation;Semantics;Graphical models;Machine learning;Training;Computer vision},
    doi = {10.1109/ICCV.2015.179}
}

@inproceedings{crf-inference,
   author = {Kr\"{a}henb\"{u}hl, Philipp and Koltun, Vladlen},
   booktitle = {Advances in Neural Information Processing Systems},
   editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},
   publisher = {Curran Associates, Inc.},
   title = {Efficient Inference in Fully Connected CRFs with Gaussian Edge Potentials},
   volume = {24},
   year = {2011},
   eprint = {1210.5644},
   archivePrefix = {arXiv}
}

@inproceedings{kaiming,
    author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
    title = {Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification},
    booktitle = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
    month = {December},
    year = {2015},
    eprint = {1502.01852},
    archivePrefix = {arXiv}
}


@InProceedings{batchnorm,
    title =     {Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift},
    author =    {Ioffe, Sergey and Szegedy, Christian},
    booktitle = {Proceedings of the 32nd International Conference on Machine Learning},
    pages =     {448--456},
    year =      {2015},
    editor =    {Bach, Francis and Blei, David},
    volume =    {37},
    series =    {Proceedings of Machine Learning Research},
    address =   {Lille, France},
    month =     {07--09 Jul},
    publisher = {PMLR},
    eprint    = {1502.03167},
    archivePrefix = {arXiv},
}

@misc{densecrf,
    author = {Philipp Kr\"{a}henb\"{u}hl, Vladlen Koltun},
    title = {DenseCRF},
    howpublished = {\url{http://www.philkr.net/}},
    year = {2011}
}

@article{dropout,
    author = {Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
    title = {Dropout: a simple way to prevent neural networks from overfitting},
    year = {2014},
    issue_date = {January 2014},
    publisher = {JMLR.org},
    volume = {15},
    number = {1},
    issn = {1532-4435},
    abstract = {Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different "thinned" networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets.},
    journal = {J. Mach. Learn. Res.},
    month = {jan},
    pages = {1929–1958},
    numpages = {30},
    keywords = {deep learning, model combination, neural networks, regularization}
}

@misc{dataset,
    author = {Dr. R. Chinnaiyan},
    title = {Fetal mri brain images dataset},
    url = {https://www.kaggle.com/datasets/vijayachinns/fetail-mri-brain-images-dataset},
    abstract = {Kaggle is the world’s largest data science community with powerful tools and resources to help you achieve your data science goals.},
    language = {en},
    urldate = {2024-04-30},
}

